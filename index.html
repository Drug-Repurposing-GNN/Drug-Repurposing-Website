<!DOCTYPE html>
<html>
<head>
    <title>FlyKD</title>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="icon" type="image/x-icon" href="/Images/pill.svg">
    <script type="text/javascript" src="app.js"></script>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
    <!-- <script src="https://cdn.jsdelivr.net/npm/chart.js"></script> -->
    <script src="https://cdn.plot.ly/plotly-2.30.0.min.js"></script>
</head>
<body>
    <button class="back-button" onclick="slideDown()"> < Back </button>
    <div id="overviewText" class="overlay-text">
        <div id="overviewBackground"></div>
        <h2>Overview:</h2>
        <p> We propose FlyKD, novel, scalable way to compress large Graph Neural Networks
            (GNN) to lighter, deployable GNNs. Specifically, FlyKD is a variation of Knowledge
            Distillation (KD) method where a larger, more capable teacher model generates
            pseudo labels for the student model to learn from. 
        </p>
        <p>
            FlyKD has two novel components in addition to the original KD to address two problems
            in Knowledge Distillation in Graphs. The first problem FlyKD addresses is memory isssue of 
            GNNs. When you generate many pseudo labels using GNNs, users will often run into cuda 
            Out of Memory (OOM) issue as the student model backpropagates. However,
            by generating unseen pseudo labels on the fly with randomness at every epoch, one can generate
            virtually infinite amount of pseudo labels for the student model to learn from. 
        </p>
        <p>
            But this poses another problem: pseudo labels are inherently noisy and difficult to optimize.
            Generating immense amount of pseudo labels worsens this problem. In order to alleviate this,
            FlyKD incorporates a form of Curriculum Learning. Inspired by how humans learn, Curriculum 
            Learning helps the optimization process of the model by introducing data at an increasing
            complexity. To incoporate Curriculum Learning, we used our prior knowledge of noisiness
            of each type of labels and gradually introduced them in the order of noisiness. 
        </p>
        <h2>PrimeKG:</h2>
        <p> 
            Our dataset is composed of the latest biomedical Knowledge Graph called Precision Medicine
            Knowledge Graph (Chandak, Huang and Zitnik (2023)), combining 20+ famous databases such 
            as DrugBank, DrugCentral, DisGeNet, etc. PrimeKG is specially designed for therepeutic 
            repurposing of drugs and can be used to find new treatments to diseases that currently 
            have no treatment. 

            PrimeKG has over 4 million relations and the scheme of the Knowledge Graph can be illustrated 
            below (adapted from Chandak, Huang and Zitnik (2023)):
            <div style="text-align: center;">
                <img src="Images/schematic.png" style="width: 40%; height: auto;">
            </div>
        </p>
        <button class="section-nav-button" id="toMethodsButton" onclick="switchSection('methodsText', 'method-text')">Methods &gt;</button>

    </div>
    <div id="methodsText" class="method-text">
        <div id="methodsBackground"></div>
        <h2>Methods:</h2>
        <p> Our approach begins by training a TxGNN model on labeled data to construct a robust teacher model, 
            which generates pseudo labels via Knowledge Distillation, thereby expanding our dataset. 
            Following this, a second TxGNN model is trained using both the original and pseudo-labeled data, 
            introducing noise elements like VGAE and Dropout to simulate complex learning scenarios. This cycle repeats, 
            refining the model with each iteration until reaching optimal stability. </p>
        <img src="Images/Labels.png" alt="Model Architecture" style="width: 40%; height: auto;">
        <p> Simultaneously, we introduce FlyKD, a novel framework that produces a vast number of pseudo labels on-the-fly, 
            overcoming traditional memory constraints. It utilizes Curriculum Learning to manage the pseudo labels' inherent noisiness effectively. 
            FlyKD creates random graphs each epoch, applying the DistMult scoring function to foster the student model's global imitation 
            of the teacher model. This iterative process, enriched with a linear loss scheduler, 
            optimizes the model's exposure to progressively complex labels, setting new standards in graph-based predictive modeling. </p>

        <h2>Resutls:</h2>
        
        <table style="width:75%; text-align: center; border-collapse: collapse; border: 1px solid black; margin-bottom: 20px;">
            <caption style="text-align: center; font-weight: bold; margin-bottom: 0.5em;">
                Baseline AUPRC (%):
            </caption>
            <tr>
                <th style="border: 1px solid black;">Model</th>
                <th style="border: 1px solid black;">Num. Params</th>
                <th style="border: 1px solid black;">Seed 45</th>
                <th style="border: 1px solid black;">Seed 46</th>
                <th style="border: 1px solid black;">Seed 47</th>
                <th style="border: 1px solid black;">Seed 48</th>
                <th style="border: 1px solid black;">Seed 49</th>
                <th style="border: 1px solid black;">Mean ± std</th>
            </tr>
            <tr>
                <td style="border: 1px solid black;">Baseline 130</td>
                <td style="border: 1px solid black;">(1.7M)</td>
                <td style="border: 1px solid black;">80.33</td>
                <td style="border: 1px solid black;">73.66</td>
                <td style="border: 1px solid black;">76.29</td>
                <td style="border: 1px solid black;">84.19</td>
                <td style="border: 1px solid black;">79.91</td>
                <td style="border: 1px solid black;">78.87 ± 4.04</td>
            </tr>
            <tr>
                <td style="border: 1px solid black;">Baseline 80</td>
                <td style="border: 1px solid black;">(650k)</td>
                <td style="border: 1px solid black;">78.64</td>
                <td style="border: 1px solid black;">71.97</td>
                <td style="border: 1px solid black;">74.44</td>
                <td style="border: 1px solid black;">82.74</td>
                <td style="border: 1px solid black;">77.87</td>
                <td style="border: 1px solid black;">77.13 ± 4.13</td>
            </tr>
        </table>
        <p>
            The zero-shot evaluation setting can lead to high variability due to different seeds. 
            To address this, we focus on the performance gains from each method relative to the baseline, averaged across seeds. 
            This way, we can more accurately attribute changes in performance to the methods themselves, 
            rather than to fluctuations in task difficulty associated with different seeds.
        </p>
        
        <table style="width:75%; margin: auto; border-collapse: collapse; border: 1px solid black; margin-bottom: 20px;">
            <caption style="text-align: center; font-weight: bold; margin-bottom: 0.5em;">
                Knowledge Distillation Methods (Relative gains from Baseline80)
            </caption>
            <tr>
                <th style="border: 1px solid black;">Model</th>
                <th style="border: 1px solid black;">Time</th>
                <th style="border: 1px solid black;">Curriculum Learning</th>
                <th style="border: 1px solid black;">Mean±std</th>
            </tr>
            <tr>
                <td style="border: 1px solid black;">Basic KD</td>
                <td style="border: 1px solid black;">1600</td>
                <td style="border: 1px solid black;">No</td>
                <td style="border: 1px solid black;">-0.62±0.59</td>
            </tr>
            <tr>
                <td style="border: 1px solid black;">LSP 1 layer (RBF)</td>
                <td style="border: 1px solid black;">20000</td>
                <td style="border: 1px solid black;">No</td>
                <td style="border: 1px solid black;">-1.09±0.23</td>
            </tr>
            <tr>
                <td style="border: 1px solid black;">LSP 2 layers (RBF)</td>
                <td style="border: 1px solid black;">40000</td>
                <td style="border: 1px solid black;">No</td>
                <td style="border: 1px solid black;">-1.41±0.82</td>
            </tr>
            <tr>
                <td style="border: 1px solid black;">FlyKD</td>
                <td style="border: 1px solid black;">2000</td>
                <td style="border: 1px solid black;">Yes</td>
                <td style="border: 1px solid black;">1.16±0.36</td>
            </tr>
        </table>
        <p>
            In our study, we delve into three distinct methods of Knowledge Distillation (KD) for Graph Neural Networks (GNN): 
            Basic Knowledge Distillation (BKD) as introduced by Hinton, Vinyals, and Dean in 2015, 
            Local Structure Preserving GCN (LSPGCN, also known as DistillGCN) developed by Yang et al. in 2020, 
            and our innovative approach, FlyKD. Our analysis reveals that while BKD and LSPGCN unfortunately result in diminished KD effects, 
            FlyKD stands out by achieving positive results compared. However, Curriculum learning was used in FlyKD, whereas it was not used
            in the other methods.
        </p>

        <!-- <div id='myDiv2'></div> -->
        <table style="width:75%; margin-left: auto; margin-right: auto; border-collapse: collapse; border: 1px solid black; margin-bottom: 20px;">
            <caption style="text-align: center; font-weight: bold; margin-bottom: 0.5em;">
                Ablation study (Relative gains from Baseline80)
            </caption>
            <tr>
                <th style="border: 1px solid black;">Model</th>
                <th style="border: 1px solid black;">Configuration</th>
                <th style="border: 1px solid black;">Mean±std</th>
            </tr>
            <tr>
                <td style="border: 1px solid black;">Basic KD</td>
                <td style="border: 1px solid black;">Employ Curriculum Learning</td>
                <td style="border: 1px solid black;">0.93±0.45</td>
            </tr>
            <tr>
                <td style="border: 1px solid black;">FlyKD</td>
                <td style="border: 1px solid black;">Fix Random Graph</td>
                <td style="border: 1px solid black;">1.14±0.39</td>
            </tr>
            <tr>
                <td style="border: 1px solid black;">FlyKD</td>
                <td style="border: 1px solid black;">No Curriculum Learning</td>
                <td style="border: 1px solid black;">0.19±0.42</td>
            </tr>
            <tr>
                <td style="border: 1px solid black;">FlyKD</td>
                <td style="border: 1px solid black;">Take Out Pseudo Labels on Train Dataset</td>
                <td style="border: 1px solid black;">-0.68±0.63</td>
            </tr>
            <tr>
                <td style="border: 1px solid black;">FlyKD</td>
                <td style="border: 1px solid black;">stepwise function for Curriculum Learning</td>
                <td style="border: 1px solid black;">-1.436±0.86</td>
            </tr>
        </table>
        <p>
            Our ablation study shows that noise in the teacher model's pseudo labels causes the performance gap between FlyKD and other KD methods. 
            Adding Curriculum Learning to BKD improves its performance noticeably, giving a +1.55% boost over the standard approach.
        </p>
        <button class="section-nav-button" id="toOverviewButton" onclick="switchSection('overviewText', 'overlay-text')">&lt; Overview</button>
    </div>
    <div class="main-container bg-image">
        <div class="centered-container">
            <div id="title-div">
                <h1 id="title">FlyKD</h1> <h1>Graph Knowledge Distillation on the Fly with Curriculum Learning</h1>
            </div>
            
            <div class="button-container">
                <button class="text-button" onclick="slideUp('overviewText', 'overlay-text')">Overview</button>
                <button class="text-button" onclick="slideUp('methodsText', 'method-text')">Methods</button>
                <button href="https://github.com/Drug-Repurposing-GNN/SSL-DiseaseDrug-Prediction" class="text-button" onclick="window.open('https://github.com/Drug-Repurposing-GNN/SSL-DiseaseDrug-Prediction')">Code</button>
            </div>
        </div>
    </div>
</body>
</html>
