<!DOCTYPE html>
<html>
<head>
    <title>FlyKD</title>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="icon" type="image/x-icon" href="/Images/pill.svg">
    <script type="text/javascript" src="app.js"></script>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
    <!-- <script src="https://cdn.jsdelivr.net/npm/chart.js"></script> -->
    <script src="https://cdn.plot.ly/plotly-2.30.0.min.js"></script>
</head>
<body>
    <button class="back-button" onclick="slideDown()"> < Back </button>
    <div id="overviewText" class="overlay-text">
        <div id="overviewBackground"></div>
        <h2>Overview:</h2>
        <p> We propose FlyKD, novel, scalable way to compress large Graph Neural Networks
            (GNN) to lighter, deployable GNNs. Specifically, FlyKD is a variation of Knowledge
            Distillation (KD) method where a larger, more capable teacher model generates
            pseudo labels for the student model to learn from. 

            FlyKD has two novel components in addition to the original KD to address two problems
            in Knowledge Distillation in Graphs. The first problem FlyKD addresses is memory isssue of 
            GNNs. When you generate many pseudo labels using the GNNs, users will often run into cuda 
            Out of Memory (OOM) issue when performing backpropagation with the student model. However,
            by generating unseen pseudo labels on the fly at every epoch, one can generate virtually
            infinite amount of pseudo labels for student model to learn from. 

            But this poses another problem, pseudo labels are inherently noisy and difficult to optimize.
            Generating immense amount of pseudo labels worsens this problem. In order to alleviate this,
            FlyKD incorporates a form of Curriculum Learning. Inspired by how humans learn, Curriculum 
            Learning helps the optimization process of the model by introduced data at an increasing
            complexity. To incoporate Curriculum Learning, we used our prior knowledge of noisiness
            of each type of labels and gradually introduced them in the order of noisiness. 

            We see that by incorporating these two novel components, FlyKD shows state-of-the-art
            performance compare to other GNN KD methods out there including the basic KD (proposed by
            Geoff Hinton et. al.) and Local Structure Preserving GCN (LSPGCN, Yang et. al.).
        </p>
        <p>
            This advancement promises to accelerate research in precision medicine and drug repurposing, 
            leveraging the strengths of TxGNN as a base while introducing novel computational strategies. 
            The collaboration of these technologies is set to transform the analysis of complex biomedical data, 
            facilitating a deeper understanding of disease mechanisms and broadening the horizons for therapeutic discovery.
        </p>
        <h2>PrimeKG:</h2>
        <p> 
            PrimeKG is a sophisticated biomedical knowledge graph that serves as a critical foundation for this project, 
            integrating an extensive range of biomedical data. It encompasses genomic information, protein interactions, 
            chemical properties, and disease associations, providing a comprehensive framework for advanced research in drug discovery 
            and precision medicine. By leveraging PrimeKG, the project taps into a rich resource of interconnected data, 
            enabling the exploration of complex biological relationships and facilitating the development of innovative 
            approaches to predictive modeling in the biomedical field.
        </p>
        
        <h2>TxGNN:</h2>
        <p> TxGNN, pivotal in our project, harnesses Graph Neural Networks to delve into PrimeKG's rich biomedical data, 
            aiming for breakthroughs in drug-disease link prediction. Its architecture is strategically designed to uncover 
            new drug applications and elucidate disease pathways, marking a significant leap toward advancing precision medicine. 
            This model not only propels our understanding of complex biological interactions 
            but also sets the stage for innovative drug discovery and repurposing initiatives.
        </p>
        <h2>Knowledge Distillation and FlyKD:</h2>
        <p> 
            FlyKD represents a cutting-edge approach to Knowledge Distillation (KD) within the realm of GNNs, 
            focusing on the biomedical domain. It transcends traditional KD techniques by generating an unlimited number of pseudo labels on-the-fly, 
            effectively overcoming memory limitations and enhancing model training efficiency. 
            Incorporating Curriculum Learning, FlyKD meticulously tackles the optimization challenges posed by noisy pseudo labels, 
            improving the model's learning curve and accuracy. This methodological innovation not only elevates the predictive modeling capabilities of 
            GNNs in biomedical research but also opens up new avenues for exploring drug-disease interactions, 
            with the potential to significantly impact therapeutic discovery and development.
        </p>
        
        <button class="section-nav-button" id="toMethodsButton" onclick="switchSection('methodsText', 'method-text')">Methods &gt;</button>

    </div>
    <div id="methodsText" class="method-text">
        <div id="methodsBackground"></div>
        <h2>Methods:</h2>
        <p> Our approach begins by training a TxGNN model on labeled data to construct a robust teacher model, 
            which generates pseudo labels via Knowledge Distillation, thereby expanding our dataset. 
            Following this, a second TxGNN model is trained using both the original and pseudo-labeled data, 
            introducing noise elements like VGAE and Dropout to simulate complex learning scenarios. This cycle repeats, 
            refining the model with each iteration until reaching optimal stability. </p>
        <img src="Images/Labels.png" alt="Model Architecture" style="width: 40%; height: auto;">
        <p> Simultaneously, we introduce FlyKD, a novel framework that produces a vast number of pseudo labels on-the-fly, 
            overcoming traditional memory constraints. It utilizes Curriculum Learning to manage the pseudo labels' inherent noisiness effectively. 
            FlyKD creates random graphs each epoch, applying the DistMult scoring function to foster the student model's global imitation 
            of the teacher model. This iterative process, enriched with a linear loss scheduler, 
            optimizes the model's exposure to progressively complex labels, setting new standards in graph-based predictive modeling. </p>

        <h2>Resutls:</h2>
        
        <!-- <canvas id="resultsChart"></canvas> -->
        <div id='myDiv'><!-- Plotly chart will be drawn inside this DIV --></div>
        <p>
            In our study, we delve into three distinct methods of Knowledge Distillation (KD) for Graph Neural Networks (GNN): 
            Basic Knowledge Distillation (BKD) as introduced by Hinton, Vinyals, and Dean in 2015, 
            Local Structure Preserving GCN (LSPGCN, also known as DistillGCN) developed by Yang et al. in 2020, 
            and our innovative approach, FlyKD. Our analysis reveals that while BKD and LSPGCN unfortunately result in diminished KD effects, 
            FlyKD stands out by achieving positive advancements beyond the foundational model, which did not incorporate KD techniques, 
            starting from a baseline of 80.
        </p>

        <div id='myDiv2'><!-- Plotly chart will be drawn inside this DIV --></div>

        <p>
            Through a comprehensive ablation study, we uncover that the disparity between FlyKD's performance 
            and that of other KD strategies can be attributed to the influence of noise in the pseudo labels generated by the teacher model. 
            Remarkably, when we integrate Curriculum Learning into the BKD framework, we witness a significant enhancement in its efficacy, 
            with a performance increase of +1.55% compared to its implementation without Curriculum Learning.
        </p>
        <button class="section-nav-button" id="toOverviewButton" onclick="switchSection('overviewText', 'overlay-text')">&lt; Overview</button>
    </div>
    <div class="main-container bg-image">
        <div class="centered-container">
            <div id="title-div">
                <h1 id="title">FlyKD</h1> <h1>Graph Knowledge Distillation on the Fly with Curriculum Learning</h1>
            </div>
            
            <div class="button-container">
                <button class="text-button" onclick="slideUp('overviewText', 'overlay-text')">Overview</button>
                <button class="text-button" onclick="slideUp('methodsText', 'method-text')">Methods</button>
                <button href="https://github.com/Drug-Repurposing-GNN/SSL-DiseaseDrug-Prediction" class="text-button" onclick="window.open('https://github.com/Drug-Repurposing-GNN/SSL-DiseaseDrug-Prediction')">Code</button>
            </div>
        </div>
    </div>
</body>
</html>
