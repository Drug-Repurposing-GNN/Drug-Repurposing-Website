<!DOCTYPE html>
<html>
<head>
    <title>FlyKD</title>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="icon" type="image/x-icon" href="/Images/pill.svg">
    <script type="text/javascript" src="app.js"></script>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
    <!-- <script src="https://cdn.jsdelivr.net/npm/chart.js"></script> -->
    <script src="https://cdn.plot.ly/plotly-2.30.0.min.js"></script>
</head>
<body>
    <button class="back-button" onclick="slideDown()"> < Back </button>
    <div id="overviewText" class="overlay-text">
        <div id="overviewBackground"></div>
        <h2>Overview:</h2>
        <p> We propose FlyKD, novel, scalable way to compress large Graph Neural Networks
            (GNN) to lighter, deployable GNNs. Specifically, FlyKD is a variation of Knowledge
            Distillation (KD) method where a larger, more capable teacher model generates
            pseudo labels for the student model to learn from. 
        </p>
        <p>
            FlyKD has two novel components in addition to the original KD to address two problems
            in Knowledge Distillation in Graphs. The first problem FlyKD addresses is memory isssue of 
            GNNs. When you generate many pseudo labels using the GNNs, users will often run into cuda 
            Out of Memory (OOM) issue when performing backpropagation with the student model. However,
            by generating unseen pseudo labels on the fly at every epoch, one can generate virtually
            infinite amount of pseudo labels for student model to learn from. 
        </p>
        <h2>PrimeKG:</h2>
        <p> 
            But this poses another problem, pseudo labels are inherently noisy and difficult to optimize.
            Generating immense amount of pseudo labels worsens this problem. In order to alleviate this,
            FlyKD incorporates a form of Curriculum Learning. Inspired by how humans learn, Curriculum 
            Learning helps the optimization process of the model by introduced data at an increasing
            complexity. To incoporate Curriculum Learning, we used our prior knowledge of noisiness
            of each type of labels and gradually introduced them in the order of noisiness. 
        </p>
        <button class="section-nav-button" id="toMethodsButton" onclick="switchSection('methodsText', 'method-text')">Methods &gt;</button>

    </div>
    <div id="methodsText" class="method-text">
        <div id="methodsBackground"></div>
        <h2>Methods:</h2>
        <p> Our approach begins by training a TxGNN model on labeled data to construct a robust teacher model, 
            which generates pseudo labels via Knowledge Distillation, thereby expanding our dataset. 
            Following this, a second TxGNN model is trained using both the original and pseudo-labeled data, 
            introducing noise elements like VGAE and Dropout to simulate complex learning scenarios. This cycle repeats, 
            refining the model with each iteration until reaching optimal stability. </p>
        <img src="Images/Labels.png" alt="Model Architecture" style="width: 40%; height: auto;">
        <p> Simultaneously, we introduce FlyKD, a novel framework that produces a vast number of pseudo labels on-the-fly, 
            overcoming traditional memory constraints. It utilizes Curriculum Learning to manage the pseudo labels' inherent noisiness effectively. 
            FlyKD creates random graphs each epoch, applying the DistMult scoring function to foster the student model's global imitation 
            of the teacher model. This iterative process, enriched with a linear loss scheduler, 
            optimizes the model's exposure to progressively complex labels, setting new standards in graph-based predictive modeling. </p>

        <h2>Resutls:</h2>
        
        <!-- <canvas id="resultsChart"></canvas> -->
        <div id='myDiv'><!-- Plotly chart will be drawn inside this DIV --></div>
        <p>
            In our study, we delve into three distinct methods of Knowledge Distillation (KD) for Graph Neural Networks (GNN): 
            Basic Knowledge Distillation (BKD) as introduced by Hinton, Vinyals, and Dean in 2015, 
            Local Structure Preserving GCN (LSPGCN, also known as DistillGCN) developed by Yang et al. in 2020, 
            and our innovative approach, FlyKD. Our analysis reveals that while BKD and LSPGCN unfortunately result in diminished KD effects, 
            FlyKD stands out by achieving positive advancements beyond the foundational model, which did not incorporate KD techniques, 
            starting from a baseline of 80.
        </p>

        <div id='myDiv2'><!-- Plotly chart will be drawn inside this DIV --></div>

        <p>
            Through a comprehensive ablation study, we uncover that the disparity between FlyKD's performance 
            and that of other KD strategies can be attributed to the influence of noise in the pseudo labels generated by the teacher model. 
            Remarkably, when we integrate Curriculum Learning into the BKD framework, we witness a significant enhancement in its efficacy, 
            with a performance increase of +1.55% compared to its implementation without Curriculum Learning.
        </p>
        <button class="section-nav-button" id="toOverviewButton" onclick="switchSection('overviewText', 'overlay-text')">&lt; Overview</button>
    </div>
    <div class="main-container bg-image">
        <div class="centered-container">
            <div id="title-div">
                <h1 id="title">FlyKD</h1> <h1>Graph Knowledge Distillation on the Fly with Curriculum Learning</h1>
            </div>
            
            <div class="button-container">
                <button class="text-button" onclick="slideUp('overviewText', 'overlay-text')">Overview</button>
                <button class="text-button" onclick="slideUp('methodsText', 'method-text')">Methods</button>
                <button href="https://github.com/Drug-Repurposing-GNN/SSL-DiseaseDrug-Prediction" class="text-button" onclick="window.open('https://github.com/Drug-Repurposing-GNN/SSL-DiseaseDrug-Prediction')">Code</button>
            </div>
        </div>
    </div>
</body>
</html>
